{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLHbwyF/4QtcfmvkJSq1RC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beamalves/TCC1/blob/main/TCC1__etapa1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script Final: Processamento, União e Análise de Capacidade com Georreferenciamento\n",
        "\n",
        "Este script integra todas as etapas em um único fluxo de trabalho:\n",
        "1. Processamento da planilha DataRecords_SUB_ONS_SINDAT\n",
        "2. União com a planilha de Capacidade Remanescente\n",
        "3. Análise e visualização dos dados combinados\n",
        "4. Exportação de shapefile para QGIS\n",
        "\n",
        "Modificações:\n",
        "- A coluna 'Nome_Planilha SINDAT ONS' contém apenas nomes que incluem \"sub\" ou \"SUB\"\n",
        "- Corrigido o erro de criação do shapefile (adicionada extensão .shp ao caminho)\n",
        "\"\"\"\n",
        "\n",
        "# Instalar todas as bibliotecas necessárias\n",
        "!pip install pandas numpy matplotlib seaborn folium unidecode openpyxl geopandas -q\n",
        "\n",
        "# Importar bibliotecas após instalação\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import unidecode\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster, HeatMap\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from google.colab import files\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Configuração inicial\n",
        "print(\"===== CONFIGURAÇÃO INICIAL =====\")\n",
        "print(\"Bibliotecas carregadas com sucesso!\")\n",
        "\n",
        "# Criar diretórios para resultados\n",
        "if not os.path.exists('resultados'):\n",
        "    os.makedirs('resultados')\n",
        "if not os.path.exists('resultados/visualizacoes'):\n",
        "    os.makedirs('resultados/visualizacoes')\n",
        "if not os.path.exists('resultados/shapefile'):\n",
        "    os.makedirs('resultados/shapefile')\n",
        "\n",
        "# Configurações de visualização\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Timer para medir tempo de execução\n",
        "tempo_inicio = time.time()\n",
        "\n",
        "# ===== ETAPA 1: PROCESSAMENTO DA PLANILHA DE COORDENADAS =====\n",
        "print(\"\\n===== ETAPA 1: PROCESSAMENTO DA PLANILHA DE COORDENADAS =====\")\n",
        "\n",
        "print(\"Faça upload do arquivo 'DataRecords_SUB_ONS_SINDAT_24abr2025.csv':\")\n",
        "uploaded_coords = files.upload()\n",
        "filename_coords = list(uploaded_coords.keys())[0]\n",
        "\n",
        "# Ler arquivo CSV\n",
        "if filename_coords.endswith('.csv'):\n",
        "    # Tentar primeiro abrir como CSV normal\n",
        "    try:\n",
        "        print(\"Tentando abrir como CSV com separador ';'...\")\n",
        "        df_coords = pd.read_csv(io.BytesIO(uploaded_coords[filename_coords]), sep=';', encoding='utf-8')\n",
        "\n",
        "        # Verificar se há uma coluna com coordenadas x,y na última coluna\n",
        "        ultima_coluna = df_coords.columns[-1]\n",
        "        print(f\"Última coluna: {ultima_coluna}\")\n",
        "\n",
        "        # Ver se a última coluna tem valores com vírgulas para x,y\n",
        "        amostra = df_coords[ultima_coluna].iloc[0] if len(df_coords) > 0 else \"\"\n",
        "        print(f\"Amostra da última coluna: {amostra}\")\n",
        "\n",
        "        if ',' in str(amostra):\n",
        "            print(\"Detectado formato de coordenadas na última coluna. Processando...\")\n",
        "            # Extrair coordenadas x e y\n",
        "            coords = df_coords[ultima_coluna].str.split(',', expand=True)\n",
        "            if len(coords.columns) >= 3:\n",
        "                df_coords['x'] = pd.to_numeric(coords[1], errors='coerce')\n",
        "                df_coords['y'] = pd.to_numeric(coords[2], errors='coerce')\n",
        "\n",
        "        # Verificar se existe a coluna 'Nome'\n",
        "        if 'Nome' in df_coords.columns:\n",
        "            print(\"Coluna 'Nome' encontrada. Processando pontos de conexão...\")\n",
        "            # Guardar o nome original da subestação\n",
        "            df_coords['Nome_Original'] = df_coords['Nome']\n",
        "\n",
        "            # Processar nomes para pontos de conexão\n",
        "            df_coords['Ponto de Conexão'] = df_coords['Nome'].apply(\n",
        "                lambda x: re.sub(r'\\bsub\\b', '', str(x), flags=re.IGNORECASE).strip() if pd.notna(x) else \"\"\n",
        "            )\n",
        "\n",
        "            # Filtrar Nome_Original para manter apenas os que contêm \"sub\" ou \"SUB\"\n",
        "            def filtrar_nome_sub(nome):\n",
        "                if pd.isna(nome):\n",
        "                    return None\n",
        "                if 'sub' in nome.lower():\n",
        "                    return nome\n",
        "                return None\n",
        "\n",
        "            df_coords['Nome_Original'] = df_coords['Nome_Original'].apply(filtrar_nome_sub)\n",
        "            print(\"Filtrando nomes originais: mantendo apenas os que contêm 'sub' ou 'SUB'\")\n",
        "\n",
        "        else:\n",
        "            print(\"ALERTA: Coluna 'Nome' não encontrada. Não será possível obter o nome original da subestação.\")\n",
        "            df_coords['Nome_Original'] = None\n",
        "\n",
        "        # Criar coluna UF se não existir\n",
        "        if 'UF' not in df_coords.columns:\n",
        "            print(\"Coluna 'UF' não encontrada. Criando coluna vazia...\")\n",
        "            df_coords['UF'] = \"\"\n",
        "\n",
        "            # Tentar extrair UF do nome\n",
        "            def extract_uf(nome):\n",
        "                # Padrões comuns de UF no final do nome\n",
        "                uf_patterns = [\n",
        "                    r'\\(([A-Z]{2})\\)$',  # UF entre parênteses no final: (SP)\n",
        "                    r'\\*([A-Z]{2})\\*$',   # UF entre asteriscos no final: *SP*\n",
        "                    r' ([A-Z]{2})$'       # UF como duas letras maiúsculas no final após espaço\n",
        "                ]\n",
        "\n",
        "                # Tentar cada padrão\n",
        "                for pattern in uf_patterns:\n",
        "                    match = re.search(pattern, nome)\n",
        "                    if match:\n",
        "                        uf = match.group(1)\n",
        "                        return uf\n",
        "\n",
        "                return \"\"\n",
        "\n",
        "            # Aplicar extração de UF se tivermos coluna de nome\n",
        "            if 'Ponto de Conexão' in df_coords.columns:\n",
        "                df_coords['UF'] = df_coords['Ponto de Conexão'].apply(extract_uf)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Se falhar, processar o arquivo manualmente\n",
        "        print(f\"Erro ao processar CSV normalmente: {e}\")\n",
        "        print(\"Processando o arquivo CSV manualmente...\")\n",
        "        raw_content = io.BytesIO(uploaded_coords[filename_coords]).read().decode('utf-8')\n",
        "        lines = raw_content.splitlines()\n",
        "\n",
        "        # Extrair cabeçalho\n",
        "        header = lines[0].split(';')\n",
        "        print(f\"Cabeçalho original: {header}\")\n",
        "\n",
        "        # Criar listas para armazenar os dados processados\n",
        "        pontos_conexao = []\n",
        "        nomes_originais = []\n",
        "        ufs = []\n",
        "        x_coords = []\n",
        "        y_coords = []\n",
        "\n",
        "        # Função para extrair UF do nome\n",
        "        def extract_uf(nome):\n",
        "            # Padrões comuns de UF no final do nome\n",
        "            uf_patterns = [\n",
        "                r'\\(([A-Z]{2})\\)$',  # UF entre parênteses no final: (SP)\n",
        "                r'\\*([A-Z]{2})\\*$',   # UF entre asteriscos no final: *SP*\n",
        "                r' ([A-Z]{2})$'       # UF como duas letras maiúsculas no final após espaço\n",
        "            ]\n",
        "\n",
        "            # Tentar cada padrão\n",
        "            for pattern in uf_patterns:\n",
        "                match = re.search(pattern, nome)\n",
        "                if match:\n",
        "                    uf = match.group(1)\n",
        "                    nome_sem_uf = re.sub(pattern, '', nome).strip()\n",
        "                    return nome_sem_uf, uf\n",
        "\n",
        "            # Se nenhum padrão foi encontrado, retornar nome original e UF vazia\n",
        "            return nome, \"\"\n",
        "\n",
        "        # Processar cada linha\n",
        "        for i in range(1, len(lines)):\n",
        "            if not lines[i].strip():\n",
        "                continue  # Pular linhas vazias\n",
        "\n",
        "            parts = lines[i].split(';')\n",
        "\n",
        "            # Obter o nome (coluna \"Nome\" é a 3ª coluna, índice 2)\n",
        "            nome = parts[2].strip() if len(parts) > 2 and parts[2] else \"\"\n",
        "\n",
        "            # Guardar o nome original, apenas se contiver \"sub\" ou \"SUB\"\n",
        "            nome_original = nome if nome and 'sub' in nome.lower() else None\n",
        "\n",
        "            # Processar as coordenadas (última coluna contém \",x,y\")\n",
        "            coords_str = parts[-1].strip() if len(parts) > 0 else \"\"\n",
        "            coords = coords_str.split(',')\n",
        "\n",
        "            # Extrair x e y se disponíveis\n",
        "            x_val = coords[1].strip() if len(coords) > 1 else \"\"\n",
        "            y_val = coords[2].strip() if len(coords) > 2 else \"\"\n",
        "\n",
        "            # Processar nome para extrair UF e remover \"sub\"\n",
        "            ponto_conexao = nome\n",
        "            if nome:\n",
        "                # Remover a palavra \"sub\" do nome\n",
        "                ponto_conexao = re.sub(r'\\bsub\\b', '', ponto_conexao, flags=re.IGNORECASE).strip()\n",
        "\n",
        "                # Extrair UF do nome\n",
        "                ponto_conexao, uf = extract_uf(ponto_conexao)\n",
        "\n",
        "                # Remover espaços múltiplos\n",
        "                ponto_conexao = re.sub(r'\\s+', ' ', ponto_conexao).strip()\n",
        "            else:\n",
        "                uf = \"\"\n",
        "\n",
        "            # Adicionar dados às listas\n",
        "            pontos_conexao.append(ponto_conexao)\n",
        "            nomes_originais.append(nome_original)\n",
        "            ufs.append(uf)\n",
        "            x_coords.append(x_val)\n",
        "            y_coords.append(y_val)\n",
        "\n",
        "        # Criar DataFrame com os dados processados\n",
        "        df_coords = pd.DataFrame({\n",
        "            'Ponto de Conexão': pontos_conexao,\n",
        "            'Nome_Original': nomes_originais,\n",
        "            'UF': ufs,\n",
        "            'x': x_coords,\n",
        "            'y': y_coords\n",
        "        })\n",
        "\n",
        "        # Converter coordenadas para tipo numérico\n",
        "        df_coords['x'] = pd.to_numeric(df_coords['x'], errors='coerce')\n",
        "        df_coords['y'] = pd.to_numeric(df_coords['y'], errors='coerce')\n",
        "else:\n",
        "    print(f\"Formato de arquivo não suportado: {filename_coords}\")\n",
        "    raise ValueError(\"Por favor, forneça um arquivo CSV.\")\n",
        "\n",
        "# Adicionar região com base na UF\n",
        "regioes_por_uf = {\n",
        "    'AC': 'Norte', 'AM': 'Norte', 'AP': 'Norte', 'PA': 'Norte',\n",
        "    'RO': 'Norte', 'RR': 'Norte', 'TO': 'Norte',\n",
        "    'AL': 'Nordeste', 'BA': 'Nordeste', 'CE': 'Nordeste', 'MA': 'Nordeste',\n",
        "    'PB': 'Nordeste', 'PE': 'Nordeste', 'PI': 'Nordeste', 'RN': 'Nordeste', 'SE': 'Nordeste',\n",
        "    'DF': 'Centro-Oeste', 'GO': 'Centro-Oeste', 'MS': 'Centro-Oeste', 'MT': 'Centro-Oeste',\n",
        "    'ES': 'Sudeste', 'MG': 'Sudeste', 'RJ': 'Sudeste', 'SP': 'Sudeste',\n",
        "    'PR': 'Sul', 'RS': 'Sul', 'SC': 'Sul'\n",
        "}\n",
        "df_coords['Regiao'] = df_coords['UF'].map(regioes_por_uf)\n",
        "\n",
        "# Verificar coordenadas válidas\n",
        "# Brasil: aproximadamente entre -73.99 e -34.79 de longitude (x) e -33.75 e 5.27 de latitude (y)\n",
        "df_coords['Coordenadas_Validas'] = (\n",
        "    (df_coords['x'] >= -73.99) & (df_coords['x'] <= -34.79) &\n",
        "    (df_coords['y'] >= -33.75) & (df_coords['y'] <= 5.27)\n",
        ")\n",
        "\n",
        "print(\"\\nProcessamento da planilha de coordenadas concluído!\")\n",
        "print(f\"Total de registros processados: {len(df_coords)}\")\n",
        "print(f\"Registros com coordenadas válidas: {df_coords['Coordenadas_Validas'].sum()} de {len(df_coords)}\")\n",
        "print(f\"Registros com UF identificada: {(df_coords['UF'] != '').sum()} de {len(df_coords)}\")\n",
        "print(f\"Registros com nome original contendo 'sub': {df_coords['Nome_Original'].notna().sum()} de {len(df_coords)}\")\n",
        "\n",
        "# Salvar o resultado intermediário\n",
        "df_coords.to_excel('resultados/DataRecords_SUB_ONS_SINDAT_processado.xlsx', index=False)\n",
        "print(\"Arquivo intermediário 'DataRecords_SUB_ONS_SINDAT_processado.xlsx' gerado na pasta 'resultados'.\")\n",
        "\n",
        "# Mostrar amostra dos dados processados\n",
        "print(\"\\nAmostra dos dados processados:\")\n",
        "print(df_coords.head())\n",
        "\n",
        "# ===== ETAPA 2: UNIÃO DAS PLANILHAS =====\n",
        "print(\"\\n===== ETAPA 2: UNIÃO DAS PLANILHAS =====\")\n",
        "\n",
        "# Upload do arquivo de capacidade remanescente\n",
        "print(\"\\nFaça upload do arquivo de capacidade remanescente (Capacidade_Remanescente_SIN_para_Escoamento_de_Geração-1_2025_24abr2025.xlsx):\")\n",
        "uploaded_cap = files.upload()\n",
        "filename_cap = list(uploaded_cap.keys())[0]\n",
        "\n",
        "# Ler arquivo de capacidade\n",
        "if filename_cap.endswith('.xlsx'):\n",
        "    df_cap = pd.read_excel(io.BytesIO(uploaded_cap[filename_cap]))\n",
        "else:\n",
        "    print(f\"Formato de arquivo não suportado: {filename_cap}\")\n",
        "    raise ValueError(\"Por favor, forneça um arquivo Excel (.xlsx).\")\n",
        "\n",
        "# Verificar estruturas dos DataFrames\n",
        "print(\"\\nEstrutura da planilha de capacidade:\")\n",
        "print(f\"Número de registros: {len(df_cap)}\")\n",
        "print(\"Colunas disponíveis:\")\n",
        "print(df_cap.columns.tolist())\n",
        "\n",
        "# Função para normalizar texto para correspondência\n",
        "def normalizar_texto(texto):\n",
        "    if pd.isna(texto) or texto == '':\n",
        "        return ''\n",
        "    # Converter para string caso não seja\n",
        "    texto = str(texto)\n",
        "    # Converter para minúsculas\n",
        "    texto = texto.lower()\n",
        "    # Remover acentos\n",
        "    texto = unidecode.unidecode(texto)\n",
        "    # Remover palavras como \"sub\", \"subestacao\", etc.\n",
        "    texto = re.sub(r'\\bsub(estacao|estação)?\\b', '', texto, flags=re.IGNORECASE)\n",
        "    texto = re.sub(r'\\bse\\b', '', texto, flags=re.IGNORECASE)\n",
        "    # Remover caracteres especiais e números\n",
        "    texto = re.sub(r'[^a-z ]', ' ', texto)\n",
        "    # Remover espaços extras\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "# Identificar coluna de ponto de conexão na planilha de capacidade\n",
        "ponto_conexao_cap = None\n",
        "for col in df_cap.columns:\n",
        "    if 'ponto' in col.lower() and 'conex' in col.lower():\n",
        "        ponto_conexao_cap = col\n",
        "        break\n",
        "\n",
        "if not ponto_conexao_cap:\n",
        "    # Tentar encontrar uma coluna que pode ter os pontos de conexão\n",
        "    possibilidades = ['Subestação', 'SE', 'Instalação', 'Nome', 'Barramento']\n",
        "    for col in possibilidades:\n",
        "        if col in df_cap.columns:\n",
        "            ponto_conexao_cap = col\n",
        "            break\n",
        "\n",
        "print(f\"\\nColuna de ponto de conexão na planilha de capacidade: {ponto_conexao_cap}\")\n",
        "\n",
        "# Verificar se a coluna Ponto de Conexão existe em df_coords\n",
        "if 'Ponto de Conexão' not in df_coords.columns:\n",
        "    print(\"ERRO: Coluna 'Ponto de Conexão' não encontrada na planilha de coordenadas!\")\n",
        "    # Tentar criar a partir de alguma outra coluna\n",
        "    if 'Nome' in df_coords.columns:\n",
        "        print(\"Criando 'Ponto de Conexão' a partir da coluna 'Nome'...\")\n",
        "        df_coords['Ponto de Conexão'] = df_coords['Nome'].apply(\n",
        "            lambda x: re.sub(r'\\bsub\\b', '', str(x), flags=re.IGNORECASE).strip() if pd.notna(x) else \"\"\n",
        "        )\n",
        "    else:\n",
        "        # Se não houver coluna adequada, usar o próprio índice\n",
        "        print(\"Nenhuma coluna de nome encontrada. Usando valores de índice para pontos de conexão...\")\n",
        "        df_coords['Ponto de Conexão'] = [f\"Ponto {i+1}\" for i in range(len(df_coords))]\n",
        "\n",
        "# Adicionar colunas normalizadas para correspondência\n",
        "df_coords['ponto_normalizado'] = df_coords['Ponto de Conexão'].apply(normalizar_texto)\n",
        "df_cap['ponto_normalizado'] = df_cap[ponto_conexao_cap].apply(normalizar_texto)\n",
        "\n",
        "# Comparar os pontos de conexão\n",
        "pontos_coords = set(df_coords['ponto_normalizado'].dropna())\n",
        "pontos_cap = set(df_cap['ponto_normalizado'].dropna())\n",
        "\n",
        "# Pontos em comum\n",
        "pontos_comuns = pontos_coords.intersection(pontos_cap)\n",
        "print(f\"\\nNúmero de pontos de conexão em comum: {len(pontos_comuns)}\")\n",
        "\n",
        "# Pontos na planilha de capacidade que não estão na planilha de coordenadas\n",
        "pontos_faltantes = pontos_cap - pontos_coords\n",
        "print(f\"Número de pontos na planilha de capacidade sem georreferenciamento: {len(pontos_faltantes)}\")\n",
        "\n",
        "# Função para encontrar a melhor correspondência usando similaridade\n",
        "def encontrar_melhor_correspondencia(nome, lista_nomes):\n",
        "    # Para nomes muito curtos, exigir correspondência exata\n",
        "    if len(nome) <= 3:\n",
        "        for n in lista_nomes:\n",
        "            if nome == n:\n",
        "                return n\n",
        "        return None\n",
        "\n",
        "    # Para nomes mais longos, usar similaridade\n",
        "    melhor_score = 0\n",
        "    melhor_match = None\n",
        "\n",
        "    for n in lista_nomes:\n",
        "        # Calcular similaridade como proporção de palavras em comum\n",
        "        palavras_nome = set(nome.split())\n",
        "        palavras_n = set(n.split())\n",
        "\n",
        "        if not palavras_nome or not palavras_n:\n",
        "            continue\n",
        "\n",
        "        palavras_comuns = palavras_nome.intersection(palavras_n)\n",
        "\n",
        "        # Calcular score como proporção de palavras em comum\n",
        "        score_nome = len(palavras_comuns) / len(palavras_nome) if len(palavras_nome) > 0 else 0\n",
        "        score_n = len(palavras_comuns) / len(palavras_n) if len(palavras_n) > 0 else 0\n",
        "        score = min(score_nome, score_n)  # Usar o menor score\n",
        "\n",
        "        if score > melhor_score and score >= 0.5:  # Exigir pelo menos 50% de similaridade\n",
        "            melhor_score = score\n",
        "            melhor_match = n\n",
        "\n",
        "    return melhor_match\n",
        "\n",
        "# Criar dicionário de correspondência para pontos não encontrados diretamente\n",
        "correspondencia_manual = {}\n",
        "lista_pontos_coords = list(pontos_coords)\n",
        "\n",
        "for ponto_cap in pontos_faltantes:\n",
        "    melhor_match = encontrar_melhor_correspondencia(ponto_cap, lista_pontos_coords)\n",
        "    if melhor_match:\n",
        "        correspondencia_manual[ponto_cap] = melhor_match\n",
        "\n",
        "print(f\"Encontradas {len(correspondencia_manual)} correspondências aproximadas para pontos faltantes\")\n",
        "\n",
        "# Realizar a junção das tabelas (correspondência direta)\n",
        "df_resultado = pd.merge(\n",
        "    df_cap,\n",
        "    df_coords[['ponto_normalizado', 'x', 'y', 'UF', 'Regiao', 'Coordenadas_Validas', 'Nome_Original']],\n",
        "    on='ponto_normalizado',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Aplicar correspondência manual para pontos faltantes\n",
        "registros_atualizados = 0\n",
        "for idx, row in df_resultado.iterrows():\n",
        "    if pd.isna(row['x']) and row['ponto_normalizado'] in correspondencia_manual:\n",
        "        ponto_correspondente = correspondencia_manual[row['ponto_normalizado']]\n",
        "        dados_coords = df_coords[df_coords['ponto_normalizado'] == ponto_correspondente]\n",
        "\n",
        "        if not dados_coords.empty:\n",
        "            df_resultado.loc[idx, 'x'] = dados_coords['x'].values[0]\n",
        "            df_resultado.loc[idx, 'y'] = dados_coords['y'].values[0]\n",
        "\n",
        "            if 'UF' in dados_coords.columns:\n",
        "                df_resultado.loc[idx, 'UF'] = dados_coords['UF'].values[0]\n",
        "\n",
        "            if 'Regiao' in dados_coords.columns:\n",
        "                df_resultado.loc[idx, 'Regiao'] = dados_coords['Regiao'].values[0]\n",
        "\n",
        "            if 'Coordenadas_Validas' in dados_coords.columns:\n",
        "                df_resultado.loc[idx, 'Coordenadas_Validas'] = dados_coords['Coordenadas_Validas'].values[0]\n",
        "\n",
        "            if 'Nome_Original' in dados_coords.columns:\n",
        "                df_resultado.loc[idx, 'Nome_Original'] = dados_coords['Nome_Original'].values[0]\n",
        "\n",
        "            registros_atualizados += 1\n",
        "\n",
        "print(f\"Atualizados {registros_atualizados} registros adicionais com correspondência aproximada\")\n",
        "\n",
        "# Verificar se há colunas duplicadas\n",
        "colunas_duplicadas = df_resultado.columns[df_resultado.columns.duplicated()]\n",
        "if len(colunas_duplicadas) > 0:\n",
        "    print(f\"\\nATENÇÃO: Foram encontradas colunas duplicadas: {list(colunas_duplicadas)}\")\n",
        "    # Remover colunas duplicadas\n",
        "    df_resultado = df_resultado.loc[:, ~df_resultado.columns.duplicated()]\n",
        "    print(\"Colunas duplicadas foram removidas.\")\n",
        "\n",
        "# Identificar e mapear as colunas necessárias para o formato final\n",
        "colunas_mapeadas = {\n",
        "    'Região': 'Regiao',\n",
        "    'UF': 'UF',\n",
        "    'Ponto de Conexão': ponto_conexao_cap,\n",
        "    'Tensão (kV)': None,\n",
        "    'N° Barra': None,\n",
        "    'PA (MW)': None,\n",
        "    'CUST (MW)': None,\n",
        "    'Ano': None,\n",
        "    'Margem (MW)': None,\n",
        "    'FatorLimitante': None,\n",
        "    'Observacao': None,\n",
        "    'Nome_Planilha SINDAT ONS': 'Nome_Original',  # Usar o nome original da subestação\n",
        "    'Coordenada X': 'x',\n",
        "    'Coordenada Y': 'y'\n",
        "}\n",
        "\n",
        "# Buscar correspondências automáticas para colunas não mapeadas\n",
        "for col_destino in colunas_mapeadas:\n",
        "    if colunas_mapeadas[col_destino] is None:\n",
        "        # Procurar colunas com nomes similares\n",
        "        for col_original in df_resultado.columns:\n",
        "            # Normalizar os nomes para comparação\n",
        "            col_dest_norm = normalizar_texto(col_destino)\n",
        "            col_orig_norm = normalizar_texto(col_original)\n",
        "\n",
        "            # Verificar se há palavras-chave em comum\n",
        "            palavras_dest = set(col_dest_norm.split())\n",
        "            palavras_orig = set(col_orig_norm.split())\n",
        "            palavras_comuns = palavras_dest.intersection(palavras_orig)\n",
        "\n",
        "            # Se houver pelo menos uma palavra em comum e for um bom candidato\n",
        "            if len(palavras_comuns) > 0 and (\n",
        "                col_dest_norm in col_orig_norm or\n",
        "                len(palavras_comuns) / len(palavras_dest) >= 0.5\n",
        "            ):\n",
        "                colunas_mapeadas[col_destino] = col_original\n",
        "                break\n",
        "\n",
        "# Mostrar o mapeamento encontrado\n",
        "print(\"\\nMapeamento de colunas:\")\n",
        "for col_destino, col_original in colunas_mapeadas.items():\n",
        "    print(f\"{col_destino} -> {col_original}\")\n",
        "\n",
        "# Criar o DataFrame final\n",
        "cols_final = [\n",
        "    'Região', 'UF', 'Ponto de Conexão', 'Tensão (kV)', 'N° Barra',\n",
        "    'PA (MW)', 'CUST (MW)', 'Ano', 'Margem (MW)', 'FatorLimitante',\n",
        "    'Observacao', 'Nome_Planilha SINDAT ONS', 'Coordenada X', 'Coordenada Y'\n",
        "]\n",
        "\n",
        "df_final = pd.DataFrame(index=df_resultado.index)\n",
        "\n",
        "# Preencher o DataFrame final\n",
        "for col_destino in cols_final:\n",
        "    col_original = colunas_mapeadas[col_destino]\n",
        "\n",
        "    if col_original and col_original in df_resultado.columns:\n",
        "        df_final[col_destino] = df_resultado[col_original]\n",
        "    else:\n",
        "        df_final[col_destino] = None  # Criar coluna vazia\n",
        "\n",
        "# Caso a coluna 'Região' não tenha sido preenchida mas temos 'UF'\n",
        "if df_final['Região'].isna().all() and not df_final['UF'].isna().all():\n",
        "    df_final['Região'] = df_final['UF'].map(regioes_por_uf)\n",
        "\n",
        "# Para a coluna Nome_Planilha SINDAT ONS, manter apenas nomes com \"sub\" ou \"SUB\"\n",
        "nome_col = 'Nome_Planilha SINDAT ONS'\n",
        "if nome_col in df_final.columns:\n",
        "    # Verificar quais valores não contêm \"sub\" ou \"SUB\"\n",
        "    sem_sub = df_final[nome_col].apply(\n",
        "        lambda x: False if pd.isna(x) else ('sub' not in str(x).lower())\n",
        "    )\n",
        "\n",
        "    # Definir esses valores como nulos\n",
        "    df_final.loc[sem_sub, nome_col] = None\n",
        "\n",
        "    print(f\"\\nFiltro aplicado na coluna '{nome_col}': mantidos apenas nomes com 'sub' ou 'SUB'\")\n",
        "    print(f\"Valores nulos após filtro: {df_final[nome_col].isna().sum()} de {len(df_final)}\")\n",
        "\n",
        "# Estatísticas finais\n",
        "print(\"\\nEstatísticas finais:\")\n",
        "print(f\"Total de registros: {len(df_final)}\")\n",
        "print(f\"Registros com coordenadas: {df_final['Coordenada X'].notna().sum()} ({df_final['Coordenada X'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Registros sem coordenadas: {df_final['Coordenada X'].isna().sum()} ({df_final['Coordenada X'].isna().sum()/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Registros com nome original de subestação: {df_final['Nome_Planilha SINDAT ONS'].notna().sum()} ({df_final['Nome_Planilha SINDAT ONS'].notna().sum()/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "# Salvar o DataFrame final\n",
        "output_filename = 'resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx'\n",
        "df_final.to_excel(output_filename, index=False)\n",
        "print(f\"\\nPlanilha final gerada: {output_filename}\")\n",
        "\n",
        "# ===== ETAPA 3: ANÁLISE E VISUALIZAÇÕES =====\n",
        "print(\"\\n===== ETAPA 3: ANÁLISE E VISUALIZAÇÕES =====\")\n",
        "\n",
        "# Converter colunas numéricas\n",
        "colunas_potencia = ['PA (MW)', 'CUST (MW)', 'Margem (MW)', 'Tensão (kV)']\n",
        "for col in colunas_potencia:\n",
        "    if col in df_final.columns:\n",
        "        df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n",
        "\n",
        "# Criar categorias para análise\n",
        "if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "    # Criar categorias de margem\n",
        "    bins = [-np.inf, 0, 100, 300, 500, np.inf]\n",
        "    labels = ['Negativa', 'Baixa (0-100)', 'Média (100-300)', 'Alta (300-500)', 'Muito Alta (>500)']\n",
        "    df_final['Categoria Margem'] = pd.cut(df_final['Margem (MW)'], bins=bins, labels=labels)\n",
        "\n",
        "    # Substituir valores NaN por \"Sem Dados\"\n",
        "    df_final['Categoria Margem'] = df_final['Categoria Margem'].cat.add_categories(['Sem Dados'])\n",
        "    df_final['Categoria Margem'] = df_final['Categoria Margem'].fillna('Sem Dados')\n",
        "\n",
        "if 'Tensão (kV)' in df_final.columns and not df_final['Tensão (kV)'].isna().all():\n",
        "    # Criar categorias de tensão\n",
        "    df_final['Categoria Tensão'] = 'Sem Dados'\n",
        "\n",
        "    # Categorias comuns de tensão no sistema elétrico brasileiro\n",
        "    df_final.loc[df_final['Tensão (kV)'] <= 69, 'Categoria Tensão'] = 'Baixa (≤69 kV)'\n",
        "    df_final.loc[(df_final['Tensão (kV)'] > 69) & (df_final['Tensão (kV)'] <= 138), 'Categoria Tensão'] = 'Média (69-138 kV)'\n",
        "    df_final.loc[(df_final['Tensão (kV)'] > 138) & (df_final['Tensão (kV)'] <= 230), 'Categoria Tensão'] = 'Alta (138-230 kV)'\n",
        "    df_final.loc[(df_final['Tensão (kV)'] > 230) & (df_final['Tensão (kV)'] <= 500), 'Categoria Tensão'] = 'Muito Alta (230-500 kV)'\n",
        "    df_final.loc[df_final['Tensão (kV)'] > 500, 'Categoria Tensão'] = 'Extra Alta (>500 kV)'\n",
        "\n",
        "# Criar análises básicas\n",
        "print(\"Gerando visualizações básicas...\")\n",
        "\n",
        "# Mapa com pontos de conexão por categoria de margem\n",
        "df_mapa = df_final.dropna(subset=['Coordenada X', 'Coordenada Y'])\n",
        "if len(df_mapa) > 0:\n",
        "    mapa = folium.Map(location=[-15.77972, -47.92972], zoom_start=5)  # Centro do Brasil\n",
        "\n",
        "    # Criar uma função para determinar a cor com base na margem\n",
        "    def get_color(margem):\n",
        "        if pd.isna(margem):\n",
        "            return 'gray'\n",
        "        elif margem < 0:\n",
        "            return 'red'\n",
        "        elif margem < 100:\n",
        "            return 'orange'\n",
        "        elif margem < 300:\n",
        "            return 'blue'\n",
        "        elif margem < 500:\n",
        "            return 'green'\n",
        "        else:\n",
        "            return 'purple'\n",
        "\n",
        "    # Adicionar legenda ao mapa\n",
        "    legend_html = \"\"\"\n",
        "    <div style=\"position: fixed;\n",
        "                bottom: 50px; left: 50px; width: 180px; height: 160px;\n",
        "                border:2px solid grey; z-index:9999; font-size:12px;\n",
        "                background-color:white;\n",
        "                padding: 10px;\n",
        "                border-radius: 5px;\">\n",
        "    <b>Legenda (Margem MW)</b><br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:red\"></i> Negativa<br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:orange\"></i> Baixa (0-100)<br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:blue\"></i> Média (100-300)<br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:green\"></i> Alta (300-500)<br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:purple\"></i> Muito Alta (>500)<br>\n",
        "    <i class=\"fa fa-circle\" style=\"color:gray\"></i> Sem Dados<br>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    mapa.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "    # Adicionar marcadores ao mapa\n",
        "    for _, row in df_mapa.iterrows():\n",
        "        cor = get_color(row['Margem (MW)'] if 'Margem (MW)' in row and pd.notna(row['Margem (MW)']) else None)\n",
        "\n",
        "        info = f\"<b>{row['Ponto de Conexão']}</b><br>\"\n",
        "        if 'Nome_Planilha SINDAT ONS' in row and pd.notna(row['Nome_Planilha SINDAT ONS']):\n",
        "            info += f\"<b>Nome Original:</b> {row['Nome_Planilha SINDAT ONS']}<br>\"\n",
        "\n",
        "        for col in ['Região', 'UF', 'Tensão (kV)', 'PA (MW)', 'CUST (MW)',\n",
        "                    'Ano', 'Margem (MW)', 'FatorLimitante']:\n",
        "            if col in df_final.columns and pd.notna(row[col]):\n",
        "                info += f\"{col}: {row[col]}<br>\"\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[row['Coordenada Y'], row['Coordenada X']],\n",
        "            radius=8,\n",
        "            popup=info,\n",
        "            tooltip=row['Ponto de Conexão'],\n",
        "            color=cor,\n",
        "            fill=True,\n",
        "            fill_color=cor\n",
        "        ).add_to(mapa)\n",
        "\n",
        "    # Salvar o mapa\n",
        "    mapa.save('resultados/visualizacoes/mapa_capacidade_pontos.html')\n",
        "    print(\"Mapa interativo gerado com sucesso!\")\n",
        "else:\n",
        "    print(\"Não há pontos com coordenadas válidas para gerar o mapa.\")\n",
        "\n",
        "# Análise por região (se houver dados suficientes)\n",
        "if 'Região' in df_final.columns and not df_final['Região'].isna().all():\n",
        "    regioes_unicas = df_final['Região'].dropna().unique()\n",
        "    if len(regioes_unicas) > 0:\n",
        "        # Contagem de pontos por região\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(data=df_final, x='Região', order=df_final['Região'].value_counts().index)\n",
        "        plt.title('Número de Pontos de Conexão por Região')\n",
        "        plt.ylabel('Número de Pontos')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('resultados/visualizacoes/contagem_por_regiao.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Análise da margem por região (se disponível)\n",
        "        if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "            # Estatísticas de margem por região\n",
        "            margem_regiao = df_final.groupby('Região')['Margem (MW)'].agg(['mean', 'min', 'max', 'count']).reset_index()\n",
        "\n",
        "            # Gráfico de barras para média de margem por região\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(data=margem_regiao, x='Região', y='mean')\n",
        "            plt.title('Margem Média (MW) por Região')\n",
        "            plt.ylabel('Margem Média (MW)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('resultados/visualizacoes/margem_media_por_regiao.png')\n",
        "            plt.close()\n",
        "\n",
        "# Análise por UF (se houver dados suficientes)\n",
        "if 'UF' in df_final.columns and not df_final['UF'].isna().all():\n",
        "    ufs_unicas = df_final['UF'].dropna().unique()\n",
        "    if len(ufs_unicas) > 0:\n",
        "        # Contagem de pontos por UF\n",
        "        contagem_uf = df_final['UF'].value_counts().head(10)  # Top 10 UFs\n",
        "\n",
        "        if len(contagem_uf) > 0:\n",
        "            # Gráfico de barras para contagem por UF (top 10)\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            sns.barplot(x=contagem_uf.index, y=contagem_uf.values)\n",
        "            plt.title('Top 10 UFs com Mais Pontos de Conexão')\n",
        "            plt.ylabel('Número de Pontos')\n",
        "            plt.xlabel('UF')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('resultados/visualizacoes/contagem_por_uf.png')\n",
        "            plt.close()\n",
        "\n",
        "# Análise por tensão (se disponível)\n",
        "if 'Tensão (kV)' in df_final.columns and not df_final['Tensão (kV)'].isna().all():\n",
        "    # Verificar se há valores válidos\n",
        "    valores_validos = df_final['Tensão (kV)'].dropna()\n",
        "    if len(valores_validos) > 0:\n",
        "        # Histograma de tensão\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valores_validos, bins=10, kde=True)\n",
        "        plt.title('Distribuição de Tensão (kV)')\n",
        "        plt.xlabel('Tensão (kV)')\n",
        "        plt.ylabel('Frequência')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('resultados/visualizacoes/distribuicao_tensao.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Gráfico de barras para contagem por categoria de tensão\n",
        "        if 'Categoria Tensão' in df_final.columns:\n",
        "            categorias_tensao = df_final['Categoria Tensão'].value_counts()\n",
        "            if len(categorias_tensao) > 0:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.countplot(data=df_final, x='Categoria Tensão', order=categorias_tensao.index)\n",
        "                plt.title('Número de Pontos por Categoria de Tensão')\n",
        "                plt.xlabel('Categoria de Tensão')\n",
        "                plt.ylabel('Número de Pontos')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('resultados/visualizacoes/contagem_por_categoria_tensao.png')\n",
        "                plt.close()\n",
        "\n",
        "# Análise de margem (se disponível)\n",
        "if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "    # Verificar se há valores válidos\n",
        "    valores_validos = df_final['Margem (MW)'].dropna()\n",
        "    if len(valores_validos) > 0:\n",
        "        # Histograma de margem\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valores_validos, bins=20, kde=True)\n",
        "        plt.title('Distribuição de Margem (MW)')\n",
        "        plt.xlabel('Margem (MW)')\n",
        "        plt.ylabel('Frequência')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('resultados/visualizacoes/distribuicao_margem.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Contagem por categoria de margem\n",
        "        if 'Categoria Margem' in df_final.columns:\n",
        "            categorias_margem = df_final['Categoria Margem'].value_counts()\n",
        "            if len(categorias_margem) > 0:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.countplot(data=df_final, x='Categoria Margem', order=categorias_margem.index)\n",
        "                plt.title('Número de Pontos por Categoria de Margem')\n",
        "                plt.xlabel('Categoria de Margem')\n",
        "                plt.ylabel('Número de Pontos')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('resultados/visualizacoes/contagem_por_categoria_margem.png')\n",
        "                plt.close()\n",
        "\n",
        "# Exportar estatísticas para Excel\n",
        "print(\"\\nExportando estatísticas para Excel...\")\n",
        "with pd.ExcelWriter('resultados/Estatisticas_Capacidade_Remanescente.xlsx') as writer:\n",
        "    # Dados gerais\n",
        "    pd.DataFrame({\n",
        "        'Métrica': ['Total de Registros', 'Registros com Coordenadas', 'Registros sem Coordenadas',\n",
        "                   'Registros com Nome SUB', 'Margem Média (MW)', 'Margem Mínima (MW)', 'Margem Máxima (MW)'],\n",
        "        'Valor': [len(df_final), df_final['Coordenada X'].notna().sum(), df_final['Coordenada X'].isna().sum(),\n",
        "                 df_final['Nome_Planilha SINDAT ONS'].notna().sum(),\n",
        "                 df_final['Margem (MW)'].mean() if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all() else None,\n",
        "                 df_final['Margem (MW)'].min() if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all() else None,\n",
        "                 df_final['Margem (MW)'].max() if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all() else None]\n",
        "    }).to_excel(writer, sheet_name='Métricas Gerais', index=False)\n",
        "\n",
        "    # Dados por região\n",
        "    if 'Região' in df_final.columns and not df_final['Região'].isna().all():\n",
        "        # Contagem por região\n",
        "        df_final.groupby('Região').size().reset_index(name='Contagem').to_excel(writer, sheet_name='Contagem por Região', index=False)\n",
        "\n",
        "        # Estatísticas de margem por região\n",
        "        if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "            df_final.groupby('Região')['Margem (MW)'].agg(['count', 'mean', 'min', 'max']).reset_index().to_excel(\n",
        "                writer, sheet_name='Margem por Região', index=False)\n",
        "\n",
        "    # Dados por UF\n",
        "    if 'UF' in df_final.columns and not df_final['UF'].isna().all():\n",
        "        # Contagem por UF\n",
        "        df_final.groupby('UF').size().reset_index(name='Contagem').sort_values('Contagem', ascending=False).to_excel(\n",
        "            writer, sheet_name='Contagem por UF', index=False)\n",
        "\n",
        "        # Estatísticas de margem por UF\n",
        "        if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "            df_final.groupby('UF')['Margem (MW)'].agg(['count', 'mean', 'min', 'max']).reset_index().sort_values('count', ascending=False).to_excel(\n",
        "                writer, sheet_name='Margem por UF', index=False)\n",
        "\n",
        "    # Dados por categoria de tensão\n",
        "    if 'Categoria Tensão' in df_final.columns:\n",
        "        # Contagem por categoria de tensão\n",
        "        df_final.groupby('Categoria Tensão').size().reset_index(name='Contagem').to_excel(\n",
        "            writer, sheet_name='Contagem por Tensão', index=False)\n",
        "\n",
        "        # Estatísticas de margem por categoria de tensão\n",
        "        if 'Margem (MW)' in df_final.columns and not df_final['Margem (MW)'].isna().all():\n",
        "            df_final.groupby('Categoria Tensão')['Margem (MW)'].agg(['count', 'mean', 'min', 'max']).reset_index().to_excel(\n",
        "                writer, sheet_name='Margem por Tensão', index=False)\n",
        "\n",
        "    # Dados por categoria de margem\n",
        "    if 'Categoria Margem' in df_final.columns:\n",
        "        # Contagem por categoria de margem\n",
        "        df_final.groupby('Categoria Margem').size().reset_index(name='Contagem').to_excel(\n",
        "            writer, sheet_name='Contagem por Cat Margem', index=False)\n",
        "\n",
        "    # Dados por fator limitante\n",
        "    if 'FatorLimitante' in df_final.columns and not df_final['FatorLimitante'].isna().all():\n",
        "        # Contagem por fator limitante\n",
        "        df_final.groupby('FatorLimitante').size().reset_index(name='Contagem').sort_values('Contagem', ascending=False).head(20).to_excel(\n",
        "            writer, sheet_name='Contagem por Fator', index=False)\n",
        "\n",
        "# ===== ETAPA 4: EXPORTAR SHAPEFILE PARA QGIS =====\n",
        "print(\"\\n===== ETAPA 4: EXPORTAR SHAPEFILE PARA QGIS =====\")\n",
        "\n",
        "# Filtrar para pontos com coordenadas válidas\n",
        "df_shp = df_final.dropna(subset=['Coordenada X', 'Coordenada Y']).copy()\n",
        "\n",
        "if len(df_shp) > 0:\n",
        "    print(f\"Criando shapefile com {len(df_shp)} pontos...\")\n",
        "\n",
        "    # Verificar se há colunas duplicadas antes de criar o GeoDataFrame\n",
        "    if df_shp.columns.duplicated().any():\n",
        "        colunas_duplicadas = df_shp.columns[df_shp.columns.duplicated()]\n",
        "        print(f\"Encontradas colunas duplicadas no dataframe: {list(colunas_duplicadas)}\")\n",
        "        print(\"Removendo colunas duplicadas...\")\n",
        "        df_shp = df_shp.loc[:, ~df_shp.columns.duplicated()]\n",
        "\n",
        "    # Criar coluna de geometria para os pontos\n",
        "    geometry = [Point(xy) for xy in zip(df_shp['Coordenada X'], df_shp['Coordenada Y'])]\n",
        "\n",
        "    # Criar GeoDataFrame com a geometria e dados\n",
        "    gdf = gpd.GeoDataFrame(df_shp, geometry=geometry, crs=\"EPSG:4326\")\n",
        "\n",
        "    # Renomear colunas longas para evitar truncamento em shapefile\n",
        "    colunas_renomeadas = {}\n",
        "    for col in gdf.columns:\n",
        "        if len(col) > 10 and col != 'geometry':\n",
        "            novo_nome = col[:10].replace(' ', '_')\n",
        "            # Garantir que o novo nome não duplica nomes existentes\n",
        "            contador = 1\n",
        "            temp_nome = novo_nome\n",
        "            while temp_nome in colunas_renomeadas.values():\n",
        "                temp_nome = f\"{novo_nome[:8]}_{contador}\"\n",
        "                contador += 1\n",
        "            colunas_renomeadas[col] = temp_nome\n",
        "\n",
        "    if colunas_renomeadas:\n",
        "        print(\"Renomeando colunas para compatibilidade com shapefile:\")\n",
        "        for col_original, col_novo in colunas_renomeadas.items():\n",
        "            print(f\"  {col_original} -> {col_novo}\")\n",
        "        gdf = gdf.rename(columns=colunas_renomeadas)\n",
        "\n",
        "    # Caminho para salvar os arquivos shapefile\n",
        "    shapefile_dir = 'resultados/shapefile'\n",
        "    shapefile_path = os.path.join(shapefile_dir, 'pontos_capacidade.shp')  # Adicionada extensão .shp\n",
        "\n",
        "    # Salvar como shapefile\n",
        "    gdf.to_file(shapefile_path, driver='ESRI Shapefile', encoding='utf-8')\n",
        "\n",
        "    print(f\"Shapefile criado com sucesso em: {shapefile_path}\")\n",
        "\n",
        "    # Compactar pasta de shapefile para download\n",
        "    !zip -r resultados/shapefile_pontos_capacidade.zip resultados/shapefile/\n",
        "\n",
        "    print(\"Shapefile compactado para download.\")\n",
        "else:\n",
        "    print(\"Não há pontos com coordenadas válidas para criar o shapefile.\")\n",
        "\n",
        "# Compactar os resultados para download\n",
        "print(\"\\nCompactando todos os resultados para download...\")\n",
        "!zip -r resultados_capacidade_remanescente.zip resultados/\n",
        "\n",
        "# Tempo de execução\n",
        "tempo_total = time.time() - tempo_inicio\n",
        "print(f\"\\nProcessamento concluído em {tempo_total:.2f} segundos!\")\n",
        "print(\"\\nArquivos gerados:\")\n",
        "print(\"- resultados/DataRecords_SUB_ONS_SINDAT_processado.xlsx\")\n",
        "print(\"- resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx\")\n",
        "print(\"- resultados/Estatisticas_Capacidade_Remanescente.xlsx\")\n",
        "print(\"- resultados/visualizacoes/ (contém gráficos e mapas)\")\n",
        "print(\"- resultados/shapefile/ (contém arquivos shapefile para QGIS)\")\n",
        "print(\"- resultados/shapefile_pontos_capacidade.zip (shapefile compactado)\")\n",
        "print(\"- resultados_capacidade_remanescente.zip (todos os resultados compactados)\")\n",
        "\n",
        "# Fazer download dos principais arquivos\n",
        "files.download('resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx')\n",
        "files.download('resultados/Estatisticas_Capacidade_Remanescente.xlsx')\n",
        "files.download('resultados/shapefile_pontos_capacidade.zip')\n",
        "files.download('resultados_capacidade_remanescente.zip')\n",
        "\n",
        "print(\"\\nProcessamento completo! Obrigado por utilizar o script.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZdv6cfQGb4D",
        "outputId": "02b053ca-49d4-439f-d895-4e1c487c2ac1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m204.8/235.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h===== CONFIGURAÇÃO INICIAL =====\n",
            "Bibliotecas carregadas com sucesso!\n",
            "\n",
            "===== ETAPA 1: PROCESSAMENTO DA PLANILHA DE COORDENADAS =====\n",
            "Faça upload do arquivo 'DataRecords_SUB_ONS_SINDAT_24abr2025.csv':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ed8fa5cf-8584-4240-9a4b-e58a23bff4c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ed8fa5cf-8584-4240-9a4b-e58a23bff4c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving DataRecords_SUB_ONS_SINDAT_24abr2025.csv to DataRecords_SUB_ONS_SINDAT_24abr2025.csv\n",
            "Tentando abrir como CSV com separador ';'...\n",
            "Última coluna: Data Entrada,x,y\n",
            "Amostra da última coluna: ,-50.283333003999985,-29.95833300099997\n",
            "Detectado formato de coordenadas na última coluna. Processando...\n",
            "Coluna 'Nome' encontrada. Processando pontos de conexão...\n",
            "Filtrando nomes originais: mantendo apenas os que contêm 'sub' ou 'SUB'\n",
            "Coluna 'UF' não encontrada. Criando coluna vazia...\n",
            "\n",
            "Processamento da planilha de coordenadas concluído!\n",
            "Total de registros processados: 1267\n",
            "Registros com coordenadas válidas: 1267 de 1267\n",
            "Registros com UF identificada: 1240 de 1267\n",
            "Registros com nome original contendo 'sub': 1227 de 1267\n",
            "Arquivo intermediário 'DataRecords_SUB_ONS_SINDAT_processado.xlsx' gerado na pasta 'resultados'.\n",
            "\n",
            "Amostra dos dados processados:\n",
            "   OBJECTID Id da Instalação Nome Id Agente Principal Agente Principal  \\\n",
            "0         1              NaN  NaN                 NaN              NaN   \n",
            "1         2              NaN  NaN                 NaN              NaN   \n",
            "2         3              NaN  NaN                 NaN              NaN   \n",
            "3         4              NaN  NaN                 NaN              NaN   \n",
            "4         5              NaN  NaN                 NaN              NaN   \n",
            "\n",
            "  Data Prevista Data Prevista.1 Data Entrada  \\\n",
            "0           NaN             NaN          NaN   \n",
            "1           NaN             NaN          NaN   \n",
            "2           NaN             NaN          NaN   \n",
            "3           NaN             NaN          NaN   \n",
            "4           NaN             NaN          NaN   \n",
            "\n",
            "                          Data Entrada,x,y          x          y  \\\n",
            "0  ,-50.283333003999985,-29.95833300099997 -50.283333 -29.958333   \n",
            "1   ,-50.30000000399997,-29.92500000499996 -50.300000 -29.925000   \n",
            "2  ,-50.25213900199998,-29.953500002999988 -50.252139 -29.953500   \n",
            "3  ,-50.255306004999966,-29.98097200799998 -50.255306 -29.980972   \n",
            "4   ,-41.12041700599997,-9.882667004999973 -41.120417  -9.882667   \n",
            "\n",
            "  Nome_Original Ponto de Conexão UF Regiao  Coordenadas_Validas  \n",
            "0          None                        NaN                 True  \n",
            "1          None                        NaN                 True  \n",
            "2          None                        NaN                 True  \n",
            "3          None                        NaN                 True  \n",
            "4          None                        NaN                 True  \n",
            "\n",
            "===== ETAPA 2: UNIÃO DAS PLANILHAS =====\n",
            "\n",
            "Faça upload do arquivo de capacidade remanescente (Capacidade_Remanescente_SIN_para_Escoamento_de_Geração-1_2025_24abr2025.xlsx):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-edfb4c3b-2410-4bd2-80fd-3a80b49946ba\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-edfb4c3b-2410-4bd2-80fd-3a80b49946ba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Capacidade_Remanescente_SIN_para_Escoamento_de_Geração-1_2025_24abr2025.xlsx to Capacidade_Remanescente_SIN_para_Escoamento_de_Geração-1_2025_24abr2025.xlsx\n",
            "\n",
            "Estrutura da planilha de capacidade:\n",
            "Número de registros: 3197\n",
            "Colunas disponíveis:\n",
            "['Região', 'UF', 'Ponto de Conexão', 'Tensão (kV)', 'N° Barra', 'PA (MW)', 'CUST (MW)', 'Ano', 'Margem (MW)', 'FatorLimitante', 'Observacao']\n",
            "\n",
            "Coluna de ponto de conexão na planilha de capacidade: Ponto de Conexão\n",
            "\n",
            "Número de pontos de conexão em comum: 5\n",
            "Número de pontos na planilha de capacidade sem georreferenciamento: 509\n",
            "Encontradas 475 correspondências aproximadas para pontos faltantes\n",
            "Atualizados 3005 registros adicionais com correspondência aproximada\n",
            "\n",
            "Mapeamento de colunas:\n",
            "Região -> Regiao\n",
            "UF -> UF\n",
            "Ponto de Conexão -> Ponto de Conexão\n",
            "Tensão (kV) -> Tensão (kV)\n",
            "N° Barra -> N° Barra\n",
            "PA (MW) -> PA (MW)\n",
            "CUST (MW) -> PA (MW)\n",
            "Ano -> Ano\n",
            "Margem (MW) -> PA (MW)\n",
            "FatorLimitante -> FatorLimitante\n",
            "Observacao -> Observacao\n",
            "Nome_Planilha SINDAT ONS -> Nome_Original\n",
            "Coordenada X -> x\n",
            "Coordenada Y -> y\n",
            "\n",
            "Filtro aplicado na coluna 'Nome_Planilha SINDAT ONS': mantidos apenas nomes com 'sub' ou 'SUB'\n",
            "Valores nulos após filtro: 162 de 3197\n",
            "\n",
            "Estatísticas finais:\n",
            "Total de registros: 3197\n",
            "Registros com coordenadas: 3035 (94.9%)\n",
            "Registros sem coordenadas: 162 (5.1%)\n",
            "Registros com nome original de subestação: 3035 (94.9%)\n",
            "\n",
            "Planilha final gerada: resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx\n",
            "\n",
            "===== ETAPA 3: ANÁLISE E VISUALIZAÇÕES =====\n",
            "Gerando visualizações básicas...\n",
            "Mapa interativo gerado com sucesso!\n",
            "\n",
            "Exportando estatísticas para Excel...\n",
            "\n",
            "===== ETAPA 4: EXPORTAR SHAPEFILE PARA QGIS =====\n",
            "Criando shapefile com 3035 pontos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-775497238eae>:790: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df_final.groupby('Categoria Margem').size().reset_index(name='Contagem').to_excel(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renomeando colunas para compatibilidade com shapefile:\n",
            "  Ponto de Conexão -> Ponto_de_C\n",
            "  Tensão (kV) -> Tensão_(kV\n",
            "  Margem (MW) -> Margem_(MW\n",
            "  FatorLimitante -> FatorLimit\n",
            "  Nome_Planilha SINDAT ONS -> Nome_Plani\n",
            "  Coordenada X -> Coordenada\n",
            "  Coordenada Y -> Coordena_1\n",
            "  Categoria Margem -> Categoria_\n",
            "  Categoria Tensão -> Categori_1\n",
            "Shapefile criado com sucesso em: resultados/shapefile/pontos_capacidade.shp\n",
            "  adding: resultados/shapefile/ (stored 0%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.dbf (deflated 97%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.shx (deflated 73%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.shp (deflated 82%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.cpg (stored 0%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.prj (deflated 17%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'Tensão_(kV' to 'Tensão_(k'\n",
            "  ogr_write(\n",
            "/usr/local/lib/python3.11/dist-packages/pyogrio/raw.py:723: RuntimeWarning: Value 'No cálculo da margem desse barramento foi considerada a UTE Marlim Azul (565 MW) conectada à SE Lagos 500 kV, e a UTE GNA I conectada de forma radial à SE Campos 2 500 kV, através do transformador 500/345 kV de 1.500 MVA na SE GNA e com os dois circuitos' of field Observacao has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
            "  ogr_write(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapefile compactado para download.\n",
            "\n",
            "Compactando todos os resultados para download...\n",
            "  adding: resultados/ (stored 0%)\n",
            "  adding: resultados/visualizacoes/ (stored 0%)\n",
            "  adding: resultados/visualizacoes/contagem_por_categoria_tensao.png (deflated 17%)\n",
            "  adding: resultados/visualizacoes/distribuicao_tensao.png (deflated 10%)\n",
            "  adding: resultados/visualizacoes/margem_media_por_regiao.png (deflated 20%)\n",
            "  adding: resultados/visualizacoes/contagem_por_categoria_margem.png (deflated 18%)\n",
            "  adding: resultados/visualizacoes/contagem_por_regiao.png (deflated 20%)\n",
            "  adding: resultados/visualizacoes/distribuicao_margem.png (deflated 19%)\n",
            "  adding: resultados/visualizacoes/contagem_por_uf.png (deflated 25%)\n",
            "  adding: resultados/visualizacoes/mapa_capacidade_pontos.html (deflated 92%)\n",
            "  adding: resultados/DataRecords_SUB_ONS_SINDAT_processado.xlsx (deflated 4%)\n",
            "  adding: resultados/shapefile/ (stored 0%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.dbf (deflated 97%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.shx (deflated 73%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.shp (deflated 82%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.cpg (stored 0%)\n",
            "  adding: resultados/shapefile/pontos_capacidade.prj (deflated 17%)\n",
            "  adding: resultados/shapefile_pontos_capacidade.zip (stored 0%)\n",
            "  adding: resultados/Estatisticas_Capacidade_Remanescente.xlsx (deflated 11%)\n",
            "  adding: resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx (deflated 5%)\n",
            "\n",
            "Processamento concluído em 53.60 segundos!\n",
            "\n",
            "Arquivos gerados:\n",
            "- resultados/DataRecords_SUB_ONS_SINDAT_processado.xlsx\n",
            "- resultados/Capacidade_Remanescente_com_Georreferenciamento.xlsx\n",
            "- resultados/Estatisticas_Capacidade_Remanescente.xlsx\n",
            "- resultados/visualizacoes/ (contém gráficos e mapas)\n",
            "- resultados/shapefile/ (contém arquivos shapefile para QGIS)\n",
            "- resultados/shapefile_pontos_capacidade.zip (shapefile compactado)\n",
            "- resultados_capacidade_remanescente.zip (todos os resultados compactados)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_65ad048b-4191-4f4b-a5f0-c3987285583d\", \"Capacidade_Remanescente_com_Georreferenciamento.xlsx\", 228562)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_87a1887d-210b-4c8a-849d-bf349ec5e6bf\", \"Estatisticas_Capacidade_Remanescente.xlsx\", 12582)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a52e0add-567c-4bef-baf5-1a0520e97785\", \"shapefile_pontos_capacidade.zip\", 123095)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b049e3d9-a0e6-42fa-bee3-09f41e848b05\", \"resultados_capacidade_remanescente.zip\", 1247365)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processamento completo! Obrigado por utilizar o script.\n"
          ]
        }
      ]
    }
  ]
}